# @package optim
# Example of PyTorch optimizer usage

defaults:
  - schema_optim
  - schema_optim_adam

optimizer:
  _target_: torch.optim.Adam
  params: ~

  weight_decay: 0
  betas: [0.9, 0.999]
  amsgrad: false

scheduler: ~
base_lr: 0.001
loss: cross